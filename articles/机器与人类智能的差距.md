<div class="inner">
<h2>机器与人类智能的差距</h2>
<p>很多人误认为“人工智能”就快实现了，是因为他们混淆了“识别”和“理解”。现在所谓的“人工智能”，离真正的智能差的实在太远。有多远呢？真正的工作几乎没有开始。</p>
<h3 id="语言">语言</h3>
<p>对于语言，人们常常混淆“语音识别”和“语言理解”，人工智能专家们很多时候还故意把两者混淆起来。所以你经常听他们说：“机器已经能理解人类语言了！” 这些都是胡扯。机器离理解人类语言其实差距非常远。</p>
<p>现在的机器可以知道你说的是哪些字，却不能理解你在说什么。即使加上所谓“知识图谱”，它也不能真的理解，因为知识图谱跟一本同义词反义词词典差不多，只不过多了一些关系。这些关系全都是文字之间的关系，它停留在文字/语法层面，而没有接触到“语义”。</p>
<p><img src="https://www.yinwang.org/images/knowledge-graph.jpg" width="80%" /></p>
<p>知识图谱的研究者们试图把词语归一下类，找找其中的关系（IS_A 之类的），以为就能够理解人类语言。可是你们想过人类是如何理解语言的吗？许多人都不知道这些词语之间有什么关系。什么近义词，反义词，IS_A…… 这些概念在小孩子的头脑里根本就没有的，可是他们却能理解你在说什么。所以知识图谱不是语言理解的解决方案。</p>
<p>要想产生语义，系统必须拥有人的“常识”，而常识是什么数据，如何获得常识，如何表示，如何利用，谁也不知道。所以理解人类语言是一个毫无头绪的工作，根本不像 AI 专家们说的“就快实现了”。</p>
<p>如果不能真的理解人类语言，什么“智能客服”，“智能个人助手”，全都只能扯淡。做成玩具忽悠小孩可能还行，真的要用来理解和处理“客服事物”，还是放弃吧。</p>
<h3 id="视觉">视觉</h3>
<p>对于视觉，AI 领域混淆了“图像识别”和“视觉理解”。深度学习视觉模型只是从大量数据拟合出的从“像素-&gt;名字”的函数，能从一堆像素“识别”出图中物体的“名字”，但它却不知道那个物体是什么。</p>
<p><img src="https://www.yinwang.org/images/ssd-road.jpg" width="80%" /></p>
<p>更严重的问题是，从像素出发是没法像人一样准确的识别物体的。人的视觉系统并不是一个“拍照+识别”的过程，而是“观察+理解+观察+理解+观察+理解……” 的过程，是一个动态的反复过程。感官穿插着理解，理解制导着观察。</p>
<p>人和动物的眼睛都是会转动的，它跟踪着感兴趣的部分。人的视觉系统能够精确地理解物体的“形状”，理解“拓扑”关系，而且这些都是 3D 的。人看到的不是像素，而是一个 3D 拓扑模型。人能理解物体的表面是“连续”的，或者有“洞”。</p>
<p>这就是为什么人可以理解抽象画，漫画，玩具。虽然世界上没有猫和老鼠长那个样子，一个从来没看过《猫和老鼠》动画片的小孩，却知道这是一只猫和一只老鼠。</p>
<p><img src="https://www.yinwang.org/images/tom-and-jerry.jpg" width="70%" /></p>
<p>你试试让一个没有用《猫和老鼠》剧照训练过的深度学习模型来识别这幅图？</p>
<p>人脑能够理解“拓扑”的概念，这使得人能够正确处理各种物体。其中特别困难的一种，是像衣服这种柔性物体。衣服是可以几乎随意变形的，上面有几个洞。“穿衣服”这种对于人类斯通见惯的事情，对于机器是无比困难的。</p>
<p>对于看到的事物，人的头脑里有一个 3D 模型，而且可以短暂模拟出这个模型的各种运动。这一切的理解都穿插在了识别物体的过程中，成为不可分割的整体。深度神经网络从来没有考虑过这些问题，又何谈实现人类级别的视觉能力呢？</p>
<p>现在的深度学习模型都是基于像素的，几乎没有抽象能力。缺乏人类视觉系统的这种“拓扑抽象理解”能力，可能就是为什么深度学习模型需要那么的多数据，而小孩子学习识别物体，根本不需要那么多数据。</p>
<p>如果没有真正的视觉理解，依赖于粗略的识别技术的“自动驾驶车”，是不可能在复杂的情况下保障人们的安全的。机器人等一系列技术，也只能停留在固定场景的“工业机器人”阶段，而不能在自然环境中自由的行动。</p>
<p>要实现真正的语言理解和视觉理解是非常困难的。真正的 AI 根本没有起步，根本没人关心，又何谈实现呢？我不是给大家泼凉水，初级的识别技术还是有用的。除非真正有人关心到问题所在，否则实现真的 AI 就只是空中楼阁。我只是提醒大家不要盲目乐观，不要被忽悠了。</p>
</div>
    